{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Web cum test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.PoseModule import PoseDetector\n",
    "\n",
    "\n",
    "def webcum():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    hands_detector = HandDetector()\n",
    "    pose_detector = PoseDetector()\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        hands, img1 = hands_detector.findHands(img)\n",
    "        print(hands)\n",
    "        img2 = pose_detector.findPose(img)\n",
    "        lmList, bboxInfo = pose_detector.findPosition(img, bboxWithHands=False)\n",
    "        print(lmList)\n",
    "        print()\n",
    "        time.sleep(2)\n",
    "        # cv2.imshow(\"CUM\", img)\n",
    "        # cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.PoseModule import PoseDetector\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mo19lj56lIbi"
   },
   "outputs": [],
   "source": [
    "class ASLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading American Sign Language (ASL) videos and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        video_folder (str): Path to the folder containing the ASL videos.\n",
    "        name_with_label (dict): A dictionary mapping video names to their labels.\n",
    "        classes (list): A list of class names.\n",
    "        transform (callable, optional): A function/transform to apply to the frames of the videos.\n",
    "\n",
    "    Attributes:\n",
    "        video_folder (str): Path to the folder containing the ASL videos.\n",
    "        name_with_label (dict): A dictionary mapping video names to their labels.\n",
    "        videos_names (list): A list of video names.\n",
    "        classes (list): A list of class names.\n",
    "        transform (callable, optional): A function/transform to apply to the frames of the videos.\n",
    "        hands_detector (HandDetector): An instance of the HandDetector class for hand detection.\n",
    "        pose_detector (PoseDetector): An instance of the PoseDetector class for pose detection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_folder, name_with_label, classes, transform=None):\n",
    "        self.video_folder = video_folder\n",
    "        self.name_with_label = name_with_label\n",
    "        self.videos_names = list(self.name_with_label.keys())\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.hands_detector = HandDetector()\n",
    "        self.pose_detector = PoseDetector()\n",
    "        self.max_frame_count = 0\n",
    "\n",
    "        # Load videos\n",
    "        self.video_tensor_sequences = self._load_all_videos()\n",
    "\n",
    "    def _load_all_videos(self):\n",
    "        \"\"\"\n",
    "        Loads all videos in advance.\n",
    "        \"\"\"\n",
    "\n",
    "        video_tensor_sequences = []\n",
    "        default_frame = [0] * (21 * 3 * 2 + 33 * 3)\n",
    "        \n",
    "        for index in range(len(self.videos_names)):\n",
    "            print(f\"{index} out of {len(self.videos_names)}\")\n",
    "            video_path = self.video_folder + '/' + self.videos_names[index] + '.mp4'\n",
    "            # Open the video file using OpenCV\n",
    "            video = cv2.VideoCapture(video_path)\n",
    "            frames_points = []\n",
    "            frame_cnt = 0\n",
    "            start_frame, end_frame = self.name_with_label[self.videos_names[index]][1], self.name_with_label[self.videos_names[index]][2]\n",
    "            while video.isOpened():\n",
    "                ret, frame = video.read()\n",
    "                frame_cnt += 1\n",
    "                # If frame inside action frames then preprocess them\n",
    "                if ret and start_frame <= frame_cnt <= end_frame:\n",
    "                    # Perform any necessary preprocessing on the frame\n",
    "                    if self.transform is not None:\n",
    "                        frame = self.transform(frame)\n",
    "                    # Collect all points. 21 points for each hand, 33 points on pose\n",
    "                    points = [0] * (21 * 3 * 2 + 33 * 3)\n",
    "                    # Recognize hands and collect them into list of all points\n",
    "                    hands, img1 = self.hands_detector.findHands(frame)\n",
    "                    for i in range(len(hands)):\n",
    "                        ind_shift = 0\n",
    "                        if hands[i].get('type') == 'Left':\n",
    "                            ind_shift = 21 * 3\n",
    "                        hand_points = hands[i].get('lmList')\n",
    "                        for j in range(len(hand_points)):\n",
    "                            for k in range(3):\n",
    "                                points[ind_shift + j * 3 + k] = hand_points[j][k]\n",
    "\n",
    "                    # Recognize the pose and collect points\n",
    "                    img2 = self.pose_detector.findPose(frame)\n",
    "                    lmList, bboxInfo = self.pose_detector.findPosition(frame, bboxWithHands=False)\n",
    "                    for i in range(len(lmList)):\n",
    "                        for j in range(1, 4):\n",
    "                            points[21 * 3 * 2 + i * 3 + j - 1] = lmList[i][j]\n",
    "                    frames_points.append(points)\n",
    "                elif not ret:\n",
    "                    break\n",
    "\n",
    "            # Release the video object\n",
    "            video.release()\n",
    "\n",
    "            # Convert the list of frames to a PyTorch tensor\n",
    "            tensor = torch.tensor(frames_points)\n",
    "            self.max_frame_count = max(self.max_frame_count, len(frames_points))\n",
    "            video_tensor_sequences.append(tensor)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of videos in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of videos in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.videos_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves a video and its corresponding label from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the video to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the video frames as a PyTorch tensor and the label.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        return self.video_tensor_sequences[index], self.name_with_label[self.videos_names[index]][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_preprocess_json(json_name, videos_root):\n",
    "    \"\"\"\n",
    "    Read json and separate videos on predefined subsets(train, val, test). Check for existence of videos.\n",
    "\n",
    "    :param json_name: path or name of json file in format {'video_name.mp4': {'subset': 'train', 'action': [class_num,\n",
    "    start_frame, end_frame]}}\n",
    "    :param videos_root: root folder of all videos\n",
    "    :return: train, validation and test dictionaries in format {'video_name.mp4': [class_num, start_frame, end_frame]}\n",
    "    \"\"\"\n",
    "    videos = json.load(open(json_name))\n",
    "    train, val, test = dict(), dict(), dict()\n",
    "    for name in os.listdir(videos_root):\n",
    "        name = name[:-4]\n",
    "        if videos.get(name) is None:\n",
    "            continue\n",
    "        if videos[name]['subset'] == 'train':\n",
    "            train[name] = videos[name]['action']\n",
    "        elif videos[name]['subset'] == 'val':\n",
    "            val[name] = videos[name]['action']\n",
    "        elif videos[name]['subset'] == 'test':\n",
    "            test[name] = videos[name]['action']\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_classes(path):\n",
    "    classes = dict()\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip().split('\\t')\n",
    "            key = int(line[0])\n",
    "            value = line[1]\n",
    "            classes[key] = value\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "6\n",
      "0 out of 32\n",
      "1 out of 32\n",
      "2 out of 32\n",
      "3 out of 32\n",
      "4 out of 32\n",
      "5 out of 32\n",
      "6 out of 32\n",
      "7 out of 32\n",
      "8 out of 32\n",
      "9 out of 32\n",
      "10 out of 32\n",
      "11 out of 32\n",
      "12 out of 32\n",
      "13 out of 32\n",
      "14 out of 32\n",
      "15 out of 32\n",
      "16 out of 32\n",
      "17 out of 32\n",
      "18 out of 32\n",
      "19 out of 32\n",
      "20 out of 32\n",
      "21 out of 32\n",
      "22 out of 32\n",
      "23 out of 32\n",
      "24 out of 32\n",
      "25 out of 32\n",
      "26 out of 32\n",
      "27 out of 32\n",
      "28 out of 32\n",
      "29 out of 32\n",
      "30 out of 32\n",
      "31 out of 32\n",
      "0 out of 6\n",
      "1 out of 6\n",
      "2 out of 6\n",
      "3 out of 6\n",
      "4 out of 6\n",
      "5 out of 6\n",
      "0 out of 7\n",
      "1 out of 7\n",
      "2 out of 7\n",
      "3 out of 7\n",
      "4 out of 7\n",
      "5 out of 7\n",
      "6 out of 7\n",
      "103\n",
      "73\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# Loading datasets (train, validation and test)\n",
    "\n",
    "# LOL TOO MUCH\n",
    "train, val, test = read_preprocess_json('wlasl_dataset/output.json', 'wlasl_dataset/videos')\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "classes = read_classes('wlasl_dataset/wlasl_class_list.txt')\n",
    "train_dataset = ASLDataset('wlasl_dataset/videos', train, classes)\n",
    "val_dataset = ASLDataset('wlasl_dataset/videos', val, classes)\n",
    "test_dataset = ASLDataset('wlasl_dataset/videos', test, classes)\n",
    "\n",
    "print(train_dataset.max_frame_count)\n",
    "print(val_dataset.max_frame_count)\n",
    "print(test_dataset.max_frame_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm = nn.Sequential(\n",
    "            nn.LSTM(1662, 64, batch_first=True),\n",
    "            nn.ReLU(),\n",
    "            nn.LSTM(64, 128, batch_first=True),\n",
    "            nn.ReLU(),\n",
    "            nn.LSTM(128, 64, batch_first=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, epochs: int, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Function that trains model using number of epochs, loss function, optimizer.\n",
    "    Can use validation or test data set for evaluation.\n",
    "    Calculates f1 score.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    model : nn.Module\n",
    "      Model to train.\n",
    "    epochs: int\n",
    "      Number of train epochs\n",
    "    criterion\n",
    "      The loss function from pytorch\n",
    "    optimizer\n",
    "      The optimizer from pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        #\n",
    "        predicted_train = []\n",
    "        true_train = []\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        bar = tqdm(train_dataloader)\n",
    "        iterations = 0\n",
    "\n",
    "        for inputs, outputs in bar:\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(inputs.to(device))\n",
    "            loss = criterion(predictions, outputs.to(device))\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get predicted classes and true classes from data\n",
    "            for item in predictions:\n",
    "                predicted_train.append(int(torch.argmax(item).cpu().numpy()))\n",
    "            for item in outputs:\n",
    "                true_train.append(int(torch.argmax(item).cpu().numpy()))\n",
    "            iterations += 1\n",
    "            bar.set_postfix(({\"loss\": f\"{train_loss/(iterations*train_dataloader.batch_size)}\"}))\n",
    "\n",
    "        # Computing loss\n",
    "        train_loss /= len(train_dataset)\n",
    "        # Computing f1 score\n",
    "        train_f1 = f1_score(true_train, predicted_train, average=\"macro\")\n",
    "\n",
    "        # Printing information in the end of train loop\n",
    "        test_loss, test_f1 = test_model(model, criterion, test_dataloader)\n",
    "        print(f\"Epoch {epoch+1} train (loss: {train_loss:.4f}, f1 score: {train_f1:.4f}) test (loss: {test_loss:.4f}, f1 score: {test_f1:.4f})\")\n",
    "\n",
    "\n",
    "def test_model(model: nn.Module, criterion, test_dataloader: DataLoader):\n",
    "    \"\"\"\n",
    "    Function that evaluates model on specified dataloader\n",
    "    by specified loss function.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    model : nn.Module\n",
    "      Model to train.\n",
    "    criterion\n",
    "      The loss function from pytorch\n",
    "    test_dataloader: DataLoader\n",
    "      The dataset for testing model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: loss of model on given dataset\n",
    "    float: f1 score of model on given dataset\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Test loss value\n",
    "    test_loss = 0.0\n",
    "\n",
    "    # Lists for calculation f1 score\n",
    "    predicted_test = []\n",
    "    true_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, outputs in test_dataloader:\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(inputs.to(device))\n",
    "            test_loss += criterion(predictions, outputs.to(device))\n",
    "\n",
    "            # Get predicted classes and true classes from data\n",
    "            for item in predictions:\n",
    "                predicted_test.append(int(torch.argmax(item).cpu().numpy()))\n",
    "            for item in outputs:\n",
    "                true_test.append(int(torch.argmax(item).cpu().numpy()))\n",
    "\n",
    "    # Computation of test loss\n",
    "    test_loss /= len(test_dataloader)\n",
    "\n",
    "    # Computation of f1 score\n",
    "    test_f1 = f1_score(true_test, predicted_test, average=\"macro\")\n",
    "    test_accuracy = accuracy_score(true_test, predicted_test)\n",
    "    return test_loss.item(), test_f1, test_accuracy\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
