{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Web cum test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.PoseModule import PoseDetector\n",
    "\n",
    "\n",
    "def webcum():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    hands_detector = HandDetector()\n",
    "    pose_detector = PoseDetector()\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        hands, img1 = hands_detector.findHands(img)\n",
    "        print(hands)\n",
    "        img2 = pose_detector.findPose(img)\n",
    "        lmList, bboxInfo = pose_detector.findPosition(img, bboxWithHands=False)\n",
    "        print(lmList)\n",
    "        print()\n",
    "        time.sleep(2)\n",
    "        # cv2.imshow(\"CUM\", img)\n",
    "        # cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['05237', '69422', '10899', '10898', '10893', '10892', '10896', '10895', '10894', '51069', '51068', '51064', '51067', '51066', '51061', '51060', '66779', '66778', '65278', '43180', '06365', '69395', '69396', '49185', '57290', '57291', '69370', '56839', '56838', '56837', '56835', '06335', '06334', '06337', '06336', '06331', '06330', '06333', '06332', '06339', '06338', '11768', '11769', '11767', '30849', '64298', '64299', '64296', '64297', '64294', '64295', '64292', '64293', '64290', '64291', '65504', '65507', '65506', '65503', '45442', '45443', '45440', '45441', '05734', '05735', '05736', '05737', '05730', '05731', '05732', '05733', '70244', '70245', '70246', '70247', '05739', '70242', '42838', '05229', '17728', '17729', '17724', '17725', '17726', '17727', '17720', '17721', '17722', '17723', '68178', '68171', '68177', '14624', '14625', '26973', '26972', '26975', '14621', '26977', '26976', '26978', '14628', '31766', '53277', '14685', '68288', '68770', '65029', '63208', '14680', '67038', '63204', '67036', '63206', '63207', '63200', '63201', '63202', '63203', '51063', '24655', '68918', '62241', '62247', '62246', '62245', '62244', '62249', '62248', '68592', '68350', '67367', '08437', '22959', '07079', '07078', '07075', '07074', '07077', '07076', '07071', '07070', '07073', '07072', '26719', '26713', '26712', '26715', '26714', '26717', '26716', '67745', '48111', '07961', '22956', '65889', '67961', '65884', '15043', '15042', '15041', '15040', '28187', '07964', '28214', '28212', '28213', '28210', '28211', '65084', '65085', '65086', '66532', '10888', '69413', '69411', '66769', '51072', '51070', '51071', '70356', '70357', '32319', '05243', '65263', '02003', '02002', '02000', '39005', '39004', '39006', '39001', '39000', '39003', '39002', '69343', '69345', '06326', '11752', '49577', '70015', '70016', '49173', '21933', '49176', '49174', '49175', '42843', '49178', '49179', '05741', '05740', '05743', '05742', '10156', '05744', '05747', '05746', '05749', '36944', '10158', '36946', '36941', '36940', '70237', '36942', '68162', '13199', '13198', '14899', '14898', '14896', '14895', '14894', '14893', '13196', '14891', '45271', '45270', '45273', '45272', '42841', '70345', '70348', '68292', '68294', '06476', '68764', '67594', '69086', '68904', '67114', '62258', '62259', '68586', '62256', '62257', '62250', '62251', '68580', '62253', '68344', '68612', '40844', '07068', '07069', '40847', '67750', '31759', '31758', '31755', '31754', '31757', '31756', '31751', '31750', '31753', '31752', '06822', '66575', '06483', '06482', '06481', '06480', '06486', '06485', '06484', '69402', '66799', '66798', '05641', '05642', '05643', '05644', '65216', '35458', '35452', '35453', '35456', '35457', '65187', '35455', '32251', '32250', '32253', '32252', '32255', '32254', '32257', '65677', '32258', '01387', '56851', '56850', '56852', '69359', '57278', '55367', '24955', '24954', '24956', '24951', '65328', '24952', '55365', '38982', '55362', '67287', '69439', '22119', '68330', '22118', '68336', '05750', '10166', '10164', '10162', '10161', '10160', '68110', '68114', '14888', '14889', '14882', '14883', '14884', '14885', '14886', '14887', '45262', '45263', '45261', '45266', '45267', '45264', '45265', '45268', '45269', '70132', '09851', '09850', '09853', '70355', '09855', '09854', '09857', '09856', '09859', '09858', '40835', '70359', '48122', '48123', '48120', '48126', '48124', '68029', '68028', '68024', '68027', '70249', '67161', '10147', '10146', '68338', '10148', '68622', '68624', '67309', '67306', '26736', '26739', '32263', '68070', '31749', '31746', '67414', '67411', '67413', '66146', '66147', '67949', '42832', '42833', '42830', '42831', '66097', '42836', '66098', '66099', '05227', '06832', '06833', '06836', '06837', '06834', '06835', '06838', '06839', '65996', '51054', '51056', '51057', '51058', '51059', '50052', '50051', '50050', '66780', '35467', '35466', '35465', '35464', '35463', '35462', '35461', '35460', '65200', '65192', '32246', '32248', '32249', '01912', '65843', '65440', '56843', '56840', '56841', '65332', '65445', '56844', '56845', '24946', '24947', '56848', '56849', '69325', '24943', '24940', '24941', '38990', '38991', '38994', '38995', '38996', '38997', '38999', '57647', '57646', '57645', '57643', '57642', '57641', '31763', '69213', '26971', '06361', '06366', '14626', '06367', '14627', '63679', '63678', '63675', '63677', '63676', '63671', '63670', '63673', '26974', '14622', '14623', '70030', '27210', '27216', '14629', '63789', '63788', '25318', '42953', '42956', '42959', '42958', '70107', '45252', '48117', '70361', '48114', '09847', '14855', '14678', '14677', '14676', '09848', '09849', '14673', '14672', '14671', '68035', '68032', '68033', '70212', '70211', '67578', '67579', '63209', '57935', '57934', '57937', '57936', '67177', '57933', '57939', '63205', '68320', '23780', '68810', '26721', '26722', '26723', '26724', '26726', '24657', '08427', '08426', '08425', '08424', '68924', '40840', '08421', '40842', '68928', '08429', '08428', '07957', '67400', '67958', '15038', '15039', '15032', '15033', '15031', '15036', '15037', '15034', '15035', '08935', '08937', '08936', '08939', '08938', '50048', '50049', '50044', '43178', '50046', '50047', '50040', '50041', '50042', '24961', '51077', '13647', '13646', '13645', '13644', '13643', '13642', '13641', '13640', '57634', '13648', '64093', '64094', '64095', '57630', '57631', '65300', '24973', '24972', '24971', '24970', '08431', '06371', '06370', '21891', '21890', '27220', '27221', '69206', '21875', '21874', '21876', '21871', '21870', '21872', '21878', '69028', '63668', '63669', '63666', '63667', '63664', '63665', '63662', '70029', '70026', '43167', '43166', '43169', '43168', '49183', '49182', '49181', '38544', '49187', '38542', '38541', '38540', '68137', '49188', '64300', '68132', '63799', '63792', '63793', '63790', '63791', '68218', '63794', '63795', '14669', '70378', '70379', '48106', '48107', '48108', '48109', '70376', '63240', '63241', '63242', '68003', '68001', '68007', '12306', '70207', '68790', '68794', '58369', '58368', '58499', '58498', '22130', '58361', '58360', '58363', '58362', '58497', '58364', '58367', '58366', '67143', '67142', '69269', '42976', '56564', '31764', '31765', '14684', '31767', '14682', '14683', '31762', '14681', '65299', '08432', '08433', '08434', '08435', '08436', '65298', '67697', '07963', '07962', '67431', '07960', '07967', '07966', '70308', '07969', '07968', '67890', '11330', '09867', '08926', '08927', '08924', '08925', '08922', '08923', '08920', '08921', '08928', '08929', '50039', '50038', '64201', '50037', '50036', '56842', '66183', '56846', '24948', '68018', '69524', '05241', '24638', '24639', '65043', '32260', '32261', '24636', '17709', '65824', '19257', '19255', '19258', '19259', '21888', '21889', '06368', '06369', '06362', '06363', '06360', '21883', '21884', '21885', '21886', '21887', '27213', '27212', '27211', '69238', '27217', '65225', '27215', '27214', '69233', '27219', '27218', '69236', '10149', '69016', '21869', '45439', '45438', '45437', '45436', '45435', '45434', '45433', '45432', '69307', '24965', '24960', '69302', '24962', '43174', '43175', '43176', '43177', '43170', '24969', '43172', '43173', '68122', '68125', '68127', '17083', '17085', '17084', '17087', '17086', '42977', '17088', '42974', '42972', '42971', '70309', '09869', '09860', '09861', '09862', '09863', '09865', '09866', '70306', '68016', '68010', '68011', '68012', '57640', '64209', '68019', '12316', '12317', '12314', '12315', '12312', '12313', '12311', '12318', '12319', '67081', '58488', '58370', '67159', '57919', '40130', '68870', '26984', '26985', '26986', '26980', '26981', '26982', '26983', '67555', '67556', '67550', '68274', '63227', '67424', '67420', '07971', '07972', '07973', '63225', '67886', '65539', '11322', '66592', '66591', '11321', '11326', '11327', '11324', '11325', '11328', '11329', '08917', '08916', '08915', '08919', '08918', '66441', '13161', '66112', '66111', '63672', '65145', '65144', '69531', '69533', '69534', '28115', '28114', '28116', '28111', '28110', '28113', '28112', '24641', '24640', '24643', '24642', '28119', '28118', '19263', '19262', '19261', '19260', '19267', '19266', '19265', '19264', '19269', '19268', '06355', '11780', '06359', '69225', '27208', '27209', '27206', '27207', '69004', '65761', '32146', '32946', '69316', '70173', '70176', '13309', '17092', '17093', '17090', '17091', '17096', '17097', '17095', '42960', '42961', '42962', '42963', '42964', '42966', '42967', '70310', '64216', '64217', '64215', '64212', '64213', '64210', '64211', '68068', '64218', '64219', '55366', '12323', '12322', '12321', '12320', '12327', '12326', '12324', '12329', '12328', '55364', '68660', '40122', '40123', '40120', '40121', '40126', '40124', '40128', '40129', '03004', '03005', '03006', '03007', '03000', '03001', '03002', '03003', '67188', '03008', '68688', '18329', '18328', '18323', '18325', '18324', '18326', '62171', '62170', '62173', '62172', '62175', '62174', '68208', '68079', '67527', '37888', '37887', '37886', '37885', '68202', '37883', '37882', '37881', '67451', '02999', '02997', '68842', '28074', '11319', '11318', '11317', '11316', '11315', '11314', '11313', '11312', '11311', '11310', '08909', '48115', '48112', '14679', '28107', '24658', '24659', '24652', '24651', '24656', '14675', '28108', '28109', '14674', '06340', '06341', '06342', '06343', '37889', '69252', '69257', '65792', '38482', '69068', '32158', '32156', '32157', '32154', '32155', '49604', '49605', '49606', '49600', '49601', '49602', '49603', '13333', '13331', '13330', '13337', '13336', '13335', '13334', '64223', '64222', '64221', '05639', '05638', '64224', '05635', '05634', '05637', '05636', '05631', '05630', '05633', '05632', '13207', '13206', '13205', '13203', '13202', '13201', '13200', '13209', '13208', '12338', '36929', '36927', '12335', '12336', '12337', '12330', '12331', '12332', '12333', '58359', '33281', '33280', '33283', '33282', '33285', '33284', '33286', '68674', '11320', '70326', '70325', '70323', '40119', '40118', '40117', '40116', '40115', '40114', '68694', '68692', '67624', '18332', '18333', '18331', '18334', '18335', '62168', '62169', '62163', '62160', '62166', '62164', '62165', '67530', '67531', '67536', '67535', '37890', '37891', '37892', '37893', '37894', '40836', '10900', '10903', '67446', '37879', '38538', '68852', '68856', '66008', '66007', '11309', '67840', '11305', '67990', '66469', '65163', '65162', '65161', '65167', '67170', '01987', '01986', '13681', '01989', '01988', '32945', '68632', '32947', '24660', '32949', '32948', '40838', '40839', '35522', '35523', '35520', '35521', '06455', '69241', '51206', '32167', '32166', '32165', '32164', '32163', '32162', '32161', '32160', '10904', '10901', '65342', '65341', '10902', '21945', '21944', '21946', '21941', '21943', '21942', '63191', '21949', '13325', '13326', '13327', '13323', '13328', '13329', '70152', '53269', '69431', '69433', '05628', '68048', '68046', '68044', '68042', '68041', '13213', '13214', '13216', '13217', '22966', '22967', '22964', '22965', '22962', '22963', '22960', '22961', '62971', '36939', '62973', '62975', '36930', '36931', '36932', '36933', '36934', '56552', '36936', '36937', '53268', '10151', '56556', '56557', '10157', '24649', '24648', '36945', '70332', '70335', '05748', '68190', '68192', '68446', '10159', '67611', '67610', '70234', '62152', '62159', '62158', '68992', '67299', '24645', '67292', '67058', '67057', '40816', '67501', '67275', '05629', '67279', '67278', '30838', '30833', '30832', '30831', '30830', '30837', '30835', '30834', '40845', '09967', '09966', '09961', '09960', '09963', '09962', '65717', '09969', '09968', '40846', '40841', '68538', '65363', '65362', '40843', '23769', '23768', '23767', '23766', '66010', '66014', '66015', '67873', '67872', '11323', '67776', '67470', '13695', '13696', '13697', '13197', '13698', '13699', '01990', '01991', '01992', '34738', '01995', '01996', '01997', '34734', '01999', '34736', '34737', '43179', '34732', '34733', '36938', '32950', '32951', '32956', '32957', '32954', '62970', '32959', '65009', '28124', '28125', '28120', '28121', '28122', '28123', '25321', '25323', '25322', '25325', '25324', '25327', '25326', '25329', '25328', '35513', '62979', '35511', '35517', '35516', '35515', '35514', '35519', '35518', '13630', '23782', '69274', '69290', '69298', '21956', '21954', '21955', '21952', '21953', '21950', '21951', '63769', '69054', '53278', '53279', '57286', '53272', '53273', '53270', '53271', '53276', '43171', '53274', '53275', '68050', '68053', '68054', '00625', '00624', '00627', '00626', '00623', '22953', '22952', '00629', '00628', '22955', '22954', '62966', '62967', '62964', '62965', '62968', '14901', '14900', '14903', '57953', '57952', '57950', '57288', '20976', '68189', '68182', '68183', '68187', '20979', '20978', '18316', '68230', '01388', '01385', '01384', '68988', '01386', '01383', '01382', '67519', '67514', '66640', '66644', '09970', '66246', '68528', '68520', '23778', '23779', '66804', '23771', '23772', '23773', '23774', '23775', '23776', '23777', '08955', '08953', '08952', '08951', '08950', '67465', '67468', '69092', '34741', '34743', '34742', '34745', '34744', '34746', '24857', '25332', '25333', '25330', '25337', '25338', '25339', '35509', '64289', '35506', '17022', '56560', '56563', '65294', '56567', '56566', '56569', '56568', '51229', '51228', '51223', '51221', '51220', '51227', '51226', '51225', '51224', '69282', '69283', '69281', '50045', '65721', '67111', '62254', '50043', '65635', '65434', '65439', '69048', '63806', '63804', '63805', '63802', '63803', '63801', '17076', '00636', '00634', '00635', '00632', '00633', '00630', '00631', '06478', '00638', '00639', '57940', '57941', '57942', '57943', '57944', '57945', '57947', '57948', '57949', '06471', '42969', '68428', '22127', '13168', '22124', '13162', '13160', '22125', '13166', '13167', '13164', '13165', '51235', '69511', '51230', '51231', '40834', '51232', '01398', '40837', '01396', '01397', '01394', '01395', '01392', '01393', '01390', '01391', '67078', '68086', '68085', '68084', '68089', '68954', '26688', '09945', '09949', '07389', '07388', '66816', '07383', '66818', '08942', '08944', '08945', '08946', '08947', '08948', '08949', '67817', '42829', '05238', '17731', '05728', '67490', '67925', '05231', '05233', '05232', '42827', '66038', '05236', '67757', '55375', '55374', '55371', '55370', '55373', '55372', '65601', '22128', '22129', '06472', '06473', '22120', '22121', '22126', '06477', '06474', '06475', '51234', '56573', '51236', '56571', '56576', '56577', '56574', '51233', '56578', '56579', '64090', '69455', '64091', '65731', '57629', '57628', '64089', '64088', '64087', '64086', '64085', '64084', '64082', '37884', '17007', '70299', '70296', '70295', '70051', '41008', '70271', '70270', '17715', '17714', '17717', '17716', '17711', '17710', '17713', '17712', '34837', '34836', '34835', '34834', '17719', '17718', '34831', '34830', '13157', '13156', '13155', '13154', '13159', '13158', '53258', '20991', '20990', '20992', '33267', '33266', '33269', '33268', '63234', '67066', '63236', '63231', '63230', '63233', '63232', '63239', '63238', '68099', '68726', '68720', '68093', '30842', '30843', '30840', '30841', '30847', '67003', '30848', '67715', '67392', '68890', '09954', '09955', '09956', '09957', '09950', '09953', '67663', '67660', '67667', '68540', '68544', '66531', '66820', '65540', '67802', '26741', '67489', '67483', '67937', '67934', '07394', '07395', '07396', '07397', '07390', '07391', '07392', '07393', '07398', '07399', '34739', '22117', '22116', '22115', '22114', '22113', '55363', '55361', '55368', '55369', '01998', '68133', '70230', '69440', '13710', '07400', '07401', '07402', '07403', '51081', '68636', '32953', '65449', '32329', '32328', '32321', '32320', '32323', '32322', '32325', '32324', '32327', '32326', '32955', '65415', '57638', '57639', '57636', '57637', '64092', '57635', '57632', '57633', '64096', '64097', '64275', '17018', '17019', '17013', '17016', '17017', '17014', '17015', '00618', '49180', '70049', '49186', '70266', '70263', '49184', '34828', '34829', '68398', '68396', '34822', '34823', '34824', '34825', '34826', '34827', '38525', '38524', '38527', '68406', '38529', '20988', '20989', '20986', '20987', '20984', '20985', '20982', '20983', '20980', '20981', '33278', '33279', '33274', '33277', '33270', '33271', '33273', '63226', '67010', '67012', '35512', '63228', '63229', '41037', '41036', '41035', '41034', '41033', '41032', '41031', '41030', '68972', '68974', '67722', '70119', '68576', '67350', '68374', '35454', '67659', '06842', '67342', '66296', '66297', '07097', '07096', '07095', '07094', '07093', '07092', '07091', '07090', '07099', '07098', '58505', '58504', '58507', '58506', '58501', '58500', '58503', '58502', '62944', '58508', '66637', '66638', '66639', '66742', '66743', '67908', '48105', '42835', '34685', '55356', '22109', '32259', '56558', '69430', '13708', '13709', '42842', '13702', '13703', '13700', '13701', '13706', '13707', '13704', '13705', '42840', '65242', '34839', '65241', '34838', '32338', '32333', '32336', '32337', '32334', '32335', '13636', '13637', '13634', '13635', '13632', '13633', '69389', '13631', '34833', '65450', '13638', '13639', '57276', '57277', '69546', '68510', '57287', '69364', '57285', '57284', '57283', '57282', '57281', '57280', '69368', '57289', '24950', '66355', '65408', '65409', '05239', '65402', '65403', '11779', '11778', '11771', '11770', '11773', '11772', '11775', '11774', '11777', '11776', '17026', '17025', '17024', '17023', '64288', '17020', '64284', '64287', '64281', '64280', '64283', '49595', '49596', '49597', '49598', '49599', '34832', '17733', '17732', '05729', '17730', '62988', '17734', '62984', '05230', '62986', '62987', '05727', '05234', '62982', '05724', '38536', '38534', '38532', '38533', '38530', '38531', '68145', '68142', '68416', '38539', '57279', '14633', '14632', '14631', '14630', '63213', '63212', '63211', '63210', '63214', '63219', '67009', '41025', '41026', '41027', '41028', '41029', '67227', '68364', '67642', '68368', '67356', '67826', '67827', '06843', '67829', '06841', '06840', '06845', '06844', '07080', '07081', '07082', '07083', '07084', '07085', '07086', '07087', '07088', '07089', '58365', '57273', '66607', '66606', '66351', '63237', '27194', '65891', '65890', '28208', '28205', '28204', '28207', '28206', '28201', '28203', '28202'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webcum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.PoseModule import PoseDetector\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "mo19lj56lIbi"
   },
   "outputs": [],
   "source": [
    "class ASLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading American Sign Language (ASL) videos and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        video_folder (str): Path to the folder containing the ASL videos.\n",
    "        name_with_label (dict): A dictionary mapping video names to their labels.\n",
    "        classes (list): A list of class names.\n",
    "        transform (callable, optional): A function/transform to apply to the frames of the videos.\n",
    "\n",
    "    Attributes:\n",
    "        video_folder (str): Path to the folder containing the ASL videos.\n",
    "        name_with_label (dict): A dictionary mapping video names to their labels.\n",
    "        videos_names (list): A list of video names.\n",
    "        classes (list): A list of class names.\n",
    "        transform (callable, optional): A function/transform to apply to the frames of the videos.\n",
    "        hands_detector (HandDetector): An instance of the HandDetector class for hand detection.\n",
    "        pose_detector (PoseDetector): An instance of the PoseDetector class for pose detection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_folder, name_with_label, classes, transform=None):\n",
    "        self.video_folder = video_folder\n",
    "        self.name_with_label = name_with_label\n",
    "        self.videos_names = list(self.name_with_label.keys())\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.hands_detector = HandDetector()\n",
    "        self.pose_detector = PoseDetector()\n",
    "        self.max_frame_count = 0\n",
    "\n",
    "        # Load videos\n",
    "        self.video_tensor_sequences = self._load_all_videos()\n",
    "\n",
    "    def _load_all_videos(self):\n",
    "        \"\"\"\n",
    "        Loads all videos in advance.\n",
    "        \"\"\"\n",
    "\n",
    "        video_tensor_sequences = []\n",
    "        default_frame = [0] * (21 * 3 * 2 + 33 * 3)\n",
    "        \n",
    "        for index in range(len(self.videos_names)):\n",
    "            print(f\"{index} out of {len(self.videos_names)}\")\n",
    "            video_path = self.video_folder + '/' + self.videos_names[index] + '.mp4'\n",
    "            # Open the video file using OpenCV\n",
    "            video = cv2.VideoCapture(video_path)\n",
    "            frames_points = []\n",
    "            frame_cnt = 0\n",
    "            start_frame, end_frame = self.name_with_label[self.videos_names[index]][1], self.name_with_label[self.videos_names[index]][2]\n",
    "            while video.isOpened():\n",
    "                ret, frame = video.read()\n",
    "                frame_cnt += 1\n",
    "                # If frame inside action frames then preprocess them\n",
    "                if ret and start_frame <= frame_cnt <= end_frame:\n",
    "                    # Perform any necessary preprocessing on the frame\n",
    "                    if self.transform is not None:\n",
    "                        frame = self.transform(frame)\n",
    "                    # Collect all points. 21 points for each hand, 33 points on pose\n",
    "                    points = [0] * (21 * 3 * 2 + 33 * 3)\n",
    "\n",
    "                    # Recognize hands and collect them into list of all points\n",
    "                    hands, img1 = self.hands_detector.findHands(frame)\n",
    "                    for i in range(len(hands)):\n",
    "                        ind_shift = 0\n",
    "                        if hands[i].get('type') == 'Left':\n",
    "                            ind_shift = 21 * 3\n",
    "                        hand_points = hands[i].get('lmList')\n",
    "                        for j in range(len(hand_points)):\n",
    "                            for k in range(3):\n",
    "                                points[ind_shift + j * 3 + k] = hand_points[j][k]\n",
    "\n",
    "                    # Recognize the pose and collect points\n",
    "                    img2 = self.pose_detector.findPose(frame)\n",
    "                    lmList, bboxInfo = self.pose_detector.findPosition(frame, bboxWithHands=False)\n",
    "                    for i in range(len(lmList)):\n",
    "                        for j in range(1, 4):\n",
    "                            points[21 * 3 * 2 + i * 3 + j - 1] = lmList[i][j]\n",
    "                    frames_points.append(points)\n",
    "                elif not ret:\n",
    "                    break\n",
    "\n",
    "            # Release the video object\n",
    "            video.release()\n",
    "\n",
    "            # Convert the list of frames to a PyTorch tensor\n",
    "            tensor = torch.tensor(frames_points)\n",
    "            self.max_frame_count = max(self.max_frame_count, len(frames_points))\n",
    "            video_tensor_sequences.append(tensor)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of videos in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of videos in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.videos_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves a video and its corresponding label from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the video to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the video frames as a PyTorch tensor and the label.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        return self.video_tesnor_sequences[index], self.name_with_label[self.videos_names[index]][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_preprocess_json(json_name, videos_root):\n",
    "    \"\"\"\n",
    "    Read json and separate videos on predefined subsets(train, val, test). Check for existence of videos.\n",
    "\n",
    "    :param json_name: path or name of json file in format {'video_name.mp4': {'subset': 'train', 'action': [class_num,\n",
    "    start_frame, end_frame]}}\n",
    "    :param videos_root: root folder of all videos\n",
    "    :return: train, validation and test dictionaries in format {'video_name.mp4': [class_num, start_frame, end_frame]}\n",
    "    \"\"\"\n",
    "    videos = json.load(open(json_name))\n",
    "    train, val, test = dict(), dict(), dict()\n",
    "    for name in os.listdir(videos_root):\n",
    "        name = name[:-4]\n",
    "        if videos.get(name) is None:\n",
    "            continue\n",
    "        if videos[name]['subset'] == 'train':\n",
    "            train[name] = videos[name]['action']\n",
    "        elif videos[name]['subset'] == 'val':\n",
    "            val[name] = videos[name]['action']\n",
    "        elif videos[name]['subset'] == 'test':\n",
    "            test[name] = videos[name]['action']\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_classes(path):\n",
    "    classes = dict()\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip().split('\\t')\n",
    "            key = int(line[0])\n",
    "            value = line[1]\n",
    "            classes[key] = value\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748\n",
      "165\n",
      "0 out of 748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(val))\n\u001b[0;32m      6\u001b[0m classes \u001b[38;5;241m=\u001b[39m read_classes(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwlasl_dataset/wlasl_class_list.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mASLDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwlasl_dataset/videos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ASLDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwlasl_dataset/videos\u001b[39m\u001b[38;5;124m'\u001b[39m, val, classes)\n\u001b[0;32m      9\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m ASLDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwlasl_dataset/videos\u001b[39m\u001b[38;5;124m'\u001b[39m, test, classes)\n",
      "Cell \u001b[1;32mIn[26], line 32\u001b[0m, in \u001b[0;36mASLDataset.__init__\u001b[1;34m(self, video_folder, name_with_label, classes, transform)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_frame_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Load videos\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_tensor_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_all_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 62\u001b[0m, in \u001b[0;36mASLDataset._load_all_videos\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m points \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m21\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m33\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Recognize hands and collect them into list of all points\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m hands, img1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhands_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindHands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(hands)):\n\u001b[0;32m     64\u001b[0m     ind_shift \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cvzone\\HandTrackingModule.py:49\u001b[0m, in \u001b[0;36mHandDetector.findHands\u001b[1;34m(self, img, draw, flipType)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03mFinds hands in a BGR image.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m:param img: Image to find the hands in.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m:param draw: Flag to draw the output on the image.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m:return: Image with or without drawings\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m imgRGB \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgRGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m allHands \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     51\u001b[0m h, w, c \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loading datasets (train, validation and test)\n",
    "\n",
    "# LOL TOO MUCH\n",
    "train, val, test = read_preprocess_json('wlasl_dataset/nslt_100.json', 'wlasl_dataset/videos')\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "classes = read_classes('wlasl_dataset/wlasl_class_list.txt')\n",
    "train_dataset = ASLDataset('wlasl_dataset/videos', train, classes)\n",
    "val_dataset = ASLDataset('wlasl_dataset/videos', val, classes)\n",
    "test_dataset = ASLDataset('wlasl_dataset/videos', test, classes)\n",
    "\n",
    "print(train_dataset.max_frame_count)\n",
    "print(val_dataset.max_frame_count)\n",
    "print(test_dataset.max_frame_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm = nn.Sequential(\n",
    "            nn.LSTM(1662, 64, batch_first=True),\n",
    "            nn.ReLU(),\n",
    "            nn.LSTM(64, 128, batch_first=True),\n",
    "            nn.ReLU(),\n",
    "            nn.LSTM(128, 64, batch_first=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, epochs: int, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Function that trains model using number of epochs, loss function, optimizer.\n",
    "    Can use validation or test data set for evaluation.\n",
    "    Calculates f1 score.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    model : nn.Module\n",
    "      Model to train.\n",
    "    epochs: int\n",
    "      Number of train epochs\n",
    "    criterion\n",
    "      The loss function from pytorch\n",
    "    optimizer\n",
    "      The optimizer from pytorch\n",
    "    \"\"\"\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        #\n",
    "        predicted_train = []\n",
    "        true_train = []\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        bar = tqdm(train_dataloader)\n",
    "        iterations = 0\n",
    "\n",
    "        for inputs, outputs in bar:\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(inputs.to(device))\n",
    "            loss = criterion(predictions, outputs.to(device))\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get predicted classes and true classes from data\n",
    "            for item in predictions:\n",
    "                predicted_train.append(int(torch.argmax(item).cpu().numpy()))\n",
    "            for item in outputs:\n",
    "                true_train.append(int(torch.argmax(item).cpu().numpy()))\n",
    "            iterations += 1\n",
    "            bar.set_postfix(({\"loss\": f\"{train_loss/(iterations*train_dataloader.batch_size)}\"}))\n",
    "\n",
    "        # Computing loss\n",
    "        train_loss /= len(train_dataset)\n",
    "        # Computing f1 score\n",
    "        train_f1 = f1_score(true_train, predicted_train, average=\"macro\")\n",
    "\n",
    "        # Printing information in the end of train loop\n",
    "        test_loss, test_f1 = test_model(model, criterion, test_dataloader)\n",
    "        print(f\"Epoch {epoch+1} train (loss: {train_loss:.4f}, f1 score: {train_f1:.4f}) test (loss: {test_loss:.4f}, f1 score: {test_f1:.4f})\")\n",
    "\n",
    "\n",
    "def test_model(model: nn.Module, criterion, test_dataloader: DataLoader):\n",
    "    \"\"\"\n",
    "    Function that evaluates model on specified dataloader\n",
    "    by specified loss function.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    model : nn.Module\n",
    "      Model to train.\n",
    "    criterion\n",
    "      The loss function from pytorch\n",
    "    test_dataloader: DataLoader\n",
    "      The dataset for testing model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float: loss of model on given dataset\n",
    "    float: f1 score of model on given dataset\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Test loss value\n",
    "    test_loss = 0.0\n",
    "\n",
    "    # Lists for calculation f1 score\n",
    "    predicted_test = []\n",
    "    true_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, outputs in test_dataloader:\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(inputs.to(device))\n",
    "            test_loss += criterion(predictions, outputs.to(device))\n",
    "\n",
    "            # Get predicted classes and true classes from data\n",
    "            for item in predictions:\n",
    "                predicted_test.append(int(torch.argmax(item).cpu().numpy()))\n",
    "            for item in outputs:\n",
    "                true_test.append(int(torch.argmax(item).cpu().numpy()))\n",
    "\n",
    "    # Computation of test loss\n",
    "    test_loss /= len(test_dataloader)\n",
    "\n",
    "    # Computation of f1 score\n",
    "    test_f1 = f1_score(true_test, predicted_test, average=\"macro\")\n",
    "    test_accuracy = accuracy_score(true_test, predicted_test)\n",
    "    return test_loss.item(), test_f1, test_accuracy\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
